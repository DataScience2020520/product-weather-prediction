{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is a notebook of helper functions that will be called for the modeling pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import scipy.sparse\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import time\n",
    "import hyperopt.pyll.stochastic\n",
    "# import mlflow\n",
    "# import hiplot as hip\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "from hyperopt import fmin, hp, tpe, Trials, SparkTrials, STATUS_OK\n",
    "from functools import partial\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, DenseVector, VectorUDT, _convert_to_vector\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StringIndexerModel, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType, BooleanType, StringType\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, \\\n",
    "                                RandomForestClassificationModel, LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_search_space = {\n",
    "                    'regParam': hp.uniform('regParam', 0, 1),\n",
    "                    'elasticNetParam': hp.uniform('elasticNetParam', 0, 1),\n",
    "                    'maxIter': hp.quniform('maxIter', 50,200, 1) #where q=1 #set to static 100\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vector_lengths(df):\n",
    "  '''\n",
    "  This function takes a PySpark dataframe and checks that each feature vector \\\n",
    "  has the same length across the whole data. \n",
    "   \n",
    "  Arguments: \n",
    "    df {PySpark Dataframe} -- PySpark dataframe containing feature fectors\n",
    "    \n",
    "  Returns:\n",
    "    1 or 0 {int} -- 1 signifies that the vector lengths are uniform and 0 that they are not\n",
    "  '''\n",
    "  \n",
    "  data_check = df\n",
    "  for feat in list(set(data_check.columns) - set(['owner'])):\n",
    "    data_check = data_check.withColumn(\"len_\"+feat, udf(lambda c: len(c))(feat))\n",
    "\n",
    "  vector_length_counts = []\n",
    "  for l in [x for x in data_check.columns if x.startswith(\"len_\")] :\n",
    "    vector_length_counts.append(data_check.groupBy(l).count().count())\n",
    "  \n",
    "  if float(np.mean(vector_length_counts))==1.0:\n",
    "    return 1\n",
    "  else:\n",
    "    data_check.groupBy(l).count().show()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_assembler(input_features, data):\n",
    "  '''\n",
    "  This function takes a PySpark dataframe and the list of input features to combine, and \\\n",
    "  creates a new column with the assembled feature vectors\n",
    "   \n",
    "  Arguments: \n",
    "    input_features {list} -- list of columns to combine using VectorAssembler\n",
    "    data {PySpark Dataframe} -- A PySpark Dataframe that will be transformed\n",
    "    \n",
    "  Returns:\n",
    "    df {PySpark Dataframe} -- A PySpark Dataframe containing a new \"features\" column\n",
    "  '''\n",
    "  \n",
    "  vector_assembler_model = VectorAssembler(\\\n",
    "  inputCols = input_features,\\\n",
    "  outputCol = \"features\")\n",
    "\n",
    "  df = vector_assembler_model.transform(data)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_indexer(df):\n",
    "  '''\n",
    "  This function takes a PySpark dataframe and converts the string values to index labels\n",
    "   \n",
    "  Arguments: \n",
    "    df {PySpark Dataframe} -- A PySpark Dataframe containing input string column\n",
    "    \n",
    "  Returns:\n",
    "    df {PySpark Dataframe} -- A PySpark Dataframe containing a new \"label\" column\n",
    "  '''\n",
    "  \n",
    "  label_indexer_model = StringIndexer(inputCol=\"owner\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "  df = label_indexer_model.fit(df).transform(df)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(trainingData):\n",
    "  '''\n",
    "  This function takes a PySpark dataframe and fits a LogisticRegression model to it\n",
    "   \n",
    "  Arguments: \n",
    "    trainingData {PySpark Dataframe} -- A PySpark Dataframe to train the model\n",
    "    \n",
    "  Returns:\n",
    "    df {PySpark Dataframe} -- A model fitted with the training data\n",
    "  '''\n",
    "  \n",
    "  lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", \\\n",
    "                          weightCol=\"weight\", elasticNetParam = 0.0, regParam = 0.0, maxIter = 100)\n",
    "  lr_model = lr.fit(trainingData)\n",
    "  trainingSummary = lr_model.summary\n",
    "  accuracy = trainingSummary.accuracy\n",
    "  \n",
    "  return lr_model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label_dict(df):\n",
    "  '''\n",
    "  This function takes a PySpark dataframe and creates a dictionary mapping the index \\\n",
    "  labels generated from StringIndexer to the original string values\n",
    "   \n",
    "  Arguments: \n",
    "    df {PySpark Dataframe} -- A PySpark Dataframe transformed using StringIndexer containing input label column\n",
    "    \n",
    "  Returns:\n",
    "    df {PySpark Dataframe} -- A dictionary with key,value pair being {label:originalLabel}\n",
    "  '''\n",
    "  \n",
    "  labelReverse = IndexToString().setInputCol(\"label\").setOutputCol('originalLabel')\n",
    "  lrtransform = labelReverse.transform(df)\n",
    "  label_mapping = lrtransform.select('label','originalLabel').dropDuplicates().orderBy('label')\n",
    "  label_dict = label_mapping.rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "  \n",
    "  return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_transform(testData, model):\n",
    "  '''\n",
    "  This function takes a fitted model and a test dataframe to generates predictions\n",
    "   \n",
    "  Arguments: \n",
    "    testData {PySpark Dataframe} -- A PySpark Dataframe to pass through the trained model\n",
    "    model {LogisticRegressionModel} -- trained model\n",
    "    \n",
    "  Returns:\n",
    "    df {PySpark Dataframe} -- A model fitted with the training data\n",
    "  '''\n",
    "  predictions = model.transform(testData)\n",
    "  \n",
    "  return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_class_weights(df):\n",
    "  '''\n",
    "  This function takes a PySpark dataframe and generates weights for each class. This \\\n",
    "  is important for handling the high imbalance in data\n",
    "   \n",
    "  Arguments: \n",
    "    df {PySpark Dataframe} -- A PySpark Dataframe containing the target classes\n",
    "    \n",
    "  Returns:\n",
    "    df {PySpark Dataframe} -- A PySpark Dataframe with a new \"weights\" column\n",
    "  '''\n",
    "  \n",
    "  y_collect = df.select(\"owner\").groupBy(\"owner\").count().collect()\n",
    "  unique_y = [x[\"owner\"] for x in y_collect]\n",
    "  total_y = sum([x[\"count\"] for x in y_collect])\n",
    "  unique_y_count = len(y_collect)\n",
    "  bin_count = [x[\"count\"] for x in y_collect]\n",
    "  class_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}\n",
    "  mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])\n",
    "  df = df.withColumn(\"weight\", mapping_expr.getItem(F.col(\"owner\")))\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(predictions):\n",
    "  '''\n",
    "  This function takes a PySpark dataframe containing model predictions and calculates \\\n",
    "  metrics to understand the performance of the model\n",
    "   \n",
    "  Arguments: \n",
    "    predictions {PySpark Dataframe} -- A PySpark Dataframe containing predicted and actual labels\n",
    "    \n",
    "  Returns:\n",
    "    metrics {tuple} -- A tuple of performance metrics in the following order: \\\n",
    "                    confusion_matrix, accuracy, fMeasure, weightedFMeasure, \\\n",
    "                    weightedFalsePositiveRate, weightedPrecision, weightedRecall\n",
    "  '''\n",
    "  \n",
    "  predictionAndLabels = predictions.rdd.map(lambda lp: (lp.prediction, lp.label))\n",
    "  metrics = MulticlassMetrics(predictionAndLabels)\n",
    "  confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "  accuracy = metrics.accuracy \n",
    "  fMeasure = metrics.fMeasure()\n",
    "  weightedFMeasure = metrics.weightedFMeasure()\n",
    "  weightedFalsePositiveRate = metrics.weightedFalsePositiveRate\n",
    "  weightedPrecision = metrics.weightedPrecision\n",
    "  weightedRecall = metrics.weightedRecall\n",
    "  \n",
    "  return confusion_matrix, accuracy, fMeasure, weightedFMeasure, weightedFalsePositiveRate, weightedPrecision, weightedRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gridsearch(data):\n",
    "  '''\n",
    "  This function takes a list of 1 or more PySpark dataframes that grid search will be performed \\\n",
    "  on to get best parameters for a Logistic Regression Classifier.\n",
    "   \n",
    "  Arguments: \n",
    "    data {list:PySpark Dataframe} -- If model_validate is set to TRUE, this list will contain (trainingData, testData) \\\n",
    "                   and if it is set to FALSE, this list will contain the whole pyspark dataframe \n",
    "    \n",
    "  Returns:\n",
    "    tvsModel.bestModel {LogisticRegressionModel} -- trained logistic regression model with most optimum parameters\n",
    "    predictions {PySpark Dataframe} -- A PySpark Dataframe containing predictions if model_validate == True\n",
    "  '''\n",
    "  \n",
    "  trainingData = data[0]\n",
    "  \n",
    "  lr = LogisticRegression(maxIter=20, regParam=0.0, elasticNetParam=0, featuresCol=\"features\", \\\n",
    "                        labelCol=\"label\", weightCol=\"weight\")\n",
    "\n",
    "  # Create ParamGrid for Cross Validation\n",
    "  paramGrid = (ParamGridBuilder()\n",
    "               .addGrid(lr.regParam, [0.0, 0.1, 0.3]) # regularization parameter\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.1]) # Elastic Net Parameter (Ridge = 0)\n",
    "               .addGrid(lr.maxIter, [20, 50, 100]) #Number of iterations\n",
    "               .build())\n",
    "  \n",
    "  evaluator = MulticlassClassificationEvaluator(metricName=\"f1\") \n",
    "\n",
    "  tvs = TrainValidationSplit(estimator=lr,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=evaluator,\n",
    "                             trainRatio=0.8)\n",
    "\n",
    "  tvsModel = tvs.fit(trainingData)\n",
    "  \n",
    "  if len(data)>1:\n",
    "    predictions = tvsModel.transform(data[1])\n",
    "    print(\"f1 Score: \", evaluator.evaluate(predictions))\n",
    "    return tvsModel.bestModel, predictions\n",
    "  else:\n",
    "    return tvsModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_matrix, predictions, normalize=True, cmap = 'winter'):\n",
    "  '''\n",
    "  This function takes a confusion matrix and dataframe of model predictions and \\\n",
    "  returns a plot of the confusion matrix where every point outside of the diagonal is \\\n",
    "  a misclassification. By default, it is normalized.\n",
    "   \n",
    "  Arguments: \n",
    "    confusion_matrix {array} -- An array representing the confusion matrix from the classifer\n",
    "    predictions {PySpark Dataframe} -- A PySpark Dataframe containing the target labels\n",
    "    normalize {bool} -- normalizes the confusion matrix for a more accurate representation\n",
    "    cmap {string} -- colormap for the plot\n",
    "    \n",
    "  Returns:\n",
    "    Display of the confusion matrix plot\n",
    "  '''\n",
    "  \n",
    "  class_temp = predictions.select(\"label\").groupBy(\"label\")\\\n",
    "                        .count().sort('count', ascending=False).toPandas()\n",
    "  class_temp = class_temp[\"label\"].values.tolist()\n",
    "\n",
    "  plt.figure()\n",
    "  if normalize:\n",
    "      confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "      title = \"Normalized confusion matrix\"\n",
    "  else:\n",
    "      title = 'Confusion matrix, without normalization'\n",
    "\n",
    "  plt.imshow(confusion_matrix, interpolation='nearest', cmap=cmap)\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(training_data, validation_data, params):\n",
    "  '''\n",
    "  This function takes the training data and hyperparameters as input and \\\n",
    "  trains a Logistic Regression Model for the chosen set of parameters. It \\\n",
    "  returns the loss, the model, as well as other information in the trials object.\n",
    "   \n",
    "  Arguments: \n",
    "    data {PySpark Dataframe} -- A PySpark Dataframe containing feature vectors and labels\n",
    "    params {dict} -- A dictionary of chosen parameter distributions from the search space\n",
    "    \n",
    "  Returns:\n",
    "    results {dict} -- loss, model, status, and runtime for that trial\n",
    "  '''\n",
    "  \n",
    "  elasticNetParam = float(params['elasticNetParam'])\n",
    "  maxIter = int(params['maxIter'])\n",
    "  regParam = float(params['regParam'])\n",
    "  \n",
    "#   (training_data, validation_data) = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "  lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", \\\n",
    "                          weightCol=\"weight\", regParam = regParam, \\\n",
    "                          elasticNetParam = elasticNetParam, maxIter = maxIter)\n",
    "  model_fitted = lr.fit(training_data)\n",
    "  model_transformed, predictions = model_transform(validation_data, model_fitted)\n",
    "\n",
    "  predictionAndLabels = predictions.rdd.map(lambda lp: (lp.prediction, lp.label))\n",
    "  metrics = MulticlassMetrics(predictionAndLabels)\n",
    "  fMeasure = metrics.fMeasure()\n",
    "  #   accuracy = metrics.accuracy() #Warning: fMeasure is deprecated, use accuracy\n",
    "  \n",
    "  return {'loss': -fMeasure, 'status': STATUS_OK, 'eval_time': time.time()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperopt(data, search_space = default_search_space, \\\n",
    "                    algo = tpe.suggest, n_evals = 32):\n",
    "  \n",
    "  '''\n",
    "  This function takes the training data as input and performs baysian hyperopt \\\n",
    "  tuning to generate the best hyperparameters and return the best model. If the \\\n",
    "  testing data is also passed then it will validate and return predictions as well.\n",
    "   \n",
    "  Arguments: \n",
    "    data {PySpark Dataframe} -- A PySpark Dataframe containing feature vectors and labels\n",
    "    search_space {dict} -- A dictionary of the parameter distributions to search from\n",
    "    algo {str} -- The search algorithm to be used. By default it is bayesian\n",
    "    n_evals {int} -- number of hyperparameter combinations to generate\n",
    "    \n",
    "  Returns:\n",
    "    results {tuple} -- If only training data was provided, it returns (trials, fitted model) \\\n",
    "                       If testing data is also provided, it returns(trials, fitted model, predictions)\n",
    "  '''\n",
    "  \n",
    "  trainData = data[0]\n",
    "  (training_data, validation_data) = trainData.randomSplit([0.8, 0.2]) \n",
    "  \n",
    "  hyperopt_training = partial(objective_func, training_data, validation_data) \n",
    "  #Use trials to distribute tuning across spark cluster\n",
    "  trials = Trials()\n",
    "  best_hyperparameters = fmin(\n",
    "    fn= hyperopt_training,\n",
    "    space=search_space,\n",
    "    algo=algo,\n",
    "    trials=trials,\n",
    "    max_evals=n_evals)\n",
    "\n",
    "  paramMap = hyperopt.space_eval(search_space, trials.argmin)\n",
    "  lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")\n",
    "  full_model = lr.fit(trainData, paramMap) #Retraining model using best parameters on full data\n",
    "  \n",
    "  if len(data)>1:\n",
    "    predictions = full_model.transform(data[1])\n",
    "    predictionAndLabels = predictions.rdd.map(lambda lp: (lp.prediction, lp.label))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    fMeasure = metrics.fMeasure()\n",
    "    return trials, full_model, predictions\n",
    "\n",
    "  else:\n",
    "    return trials, full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline(data, datestamp, train_existing_model = True, hyperparameter_tuning = False, \\\n",
    "                      training_split = 0.8, model_validate = True, evaluate_metrics = False):\n",
    "  '''\n",
    "  This is the main function to be called for executing the model training pipeline. It takes \\\n",
    "  a PySpark dataframe which is first transformed, and trained using existing model or performs \\\n",
    "  hyperparameter tuning, following which the prediction metrics are evaluated.\n",
    "   \n",
    "  Arguments: \n",
    "    data {PySpark Dataframe} -- A PySpark Dataframe containing feature vectors and labels\n",
    "    train_existing_model {bool} -- If TRUE, the latest model parameters are used for training\n",
    "    hyperparameter_tuning {bool} -- If TRUE, a grid search is performed to get the optimum parameters\n",
    "    training_split {float} -- fraction in range (0,1) that will be the training data after randomSplit\n",
    "    model_validate {bool} -- If TRUE, it splits the dataset into (train, test) using randomSplit\n",
    "                             If FALSE, it passes the whole data into the model and generates no predictions\n",
    "    evaluate_metrics {bool} -- If TRUE, it calculates the performance metrics of a model predictions\n",
    "    \n",
    "  Returns:\n",
    "    results {tuple} -- A tuple containing results in the following order:\n",
    "                    (model, label_dict, trained_owners_dict, predictions, metrics, classificationReport)\n",
    "                    If all boolean parameters were FALSE, it will return only (df, label_dict)\n",
    "  '''\n",
    "  \n",
    "  if check_vector_lengths(data)==0:\n",
    "    sys.exit('Feature vector lengths are different')\n",
    "    \n",
    "  label_dict_path = '/dbfs/dbfs/FileStore/label_dicts/label_mapping_{}.pkl'.format(datestamp)\n",
    "  trials_path = '/dbfs/dbfs/FileStore/metrics/trials_{}.pkl'.format(datestamp)\n",
    "  model_path = '/dbfs/FileStore/models/model_{}.sav'.format(datestamp)\n",
    "#   validation_predictions_path = '/dbfs/dbfs/FileStore/predictions/validation_predictions_{}.csv'.format(datestamp)\n",
    "  validation_predictions_tablename = 'validation_predictions_{}'.format(datestamp)\n",
    "  trained_owners_dict_path = '/dbfs/dbfs/FileStore/trained_owners/trained_owners_{}.pkl'.format(datestamp)\n",
    "  metrics_path = '/dbfs/dbfs/FileStore/metrics/validation_metrics_{}.pkl'.format(datestamp)\n",
    "  classification_report_path = '/dbfs/dbfs/FileStore/metrics/validation_classification_report_{}.pkl'.format(datestamp)\n",
    "  transformed_df_path = 'transformed_df_{}'.format(datestamp)\n",
    "  results = {}\n",
    "  \n",
    "  input_features = list(set(data.columns) - set(['owner','vulnerability_id']))\n",
    "  df = vector_assembler(input_features, data).select('features','owner','vulnerability_id')\n",
    "  df = label_indexer(df)\n",
    "  label_dict = build_label_dict(df)\n",
    "  pkl.dump(label_dict, open(label_dict_path, 'wb'))\n",
    "  df = add_class_weights(df)\n",
    "  \n",
    "  if model_validate:\n",
    "    (trainingData, testData) = df.randomSplit([training_split, 1 - training_split])\n",
    "    if hyperparameter_tuning:\n",
    "      trials, model, predictions = optimize_hyperopt([trainingData, testData])\n",
    "      results['training_accuracy'] = trials.best_trial['result']['loss']*-1\n",
    "      pkl.dump(trials, open(trials_path, 'wb'))\n",
    "      model.save(model_path)\n",
    "    elif train_existing_model:\n",
    "      model_fitted, accuracy = model_fit(trainingData)\n",
    "      results['training_accuracy'] = accuracy\n",
    "      model, predictions = model_transform(testData, model_fitted)\n",
    "      model.save(model_path)\n",
    "    else:\n",
    "      print(\"Please make one of these parameters True: [hyperparameter_tuning, existing_model] \\\n",
    "             \\n OR make model_validate == False\")\n",
    "    predictions = predictions.withColumn('ml_predicted_owner', udf(lambda x: label_dict[x])('prediction'))\n",
    "    predictions = predictions.withColumn('array_probs', udf(lambda v: v.toArray().tolist(), \\\n",
    "                                                        ArrayType(FloatType()))('probability'))\n",
    "    predictions = predictions.withColumn('confidence_factor', udf(lambda x: max(x), FloatType())('array_probs'))\n",
    "    predictions = predictions.withColumn('ml_owner',udf(lambda x: x)('ml_predicted_owner'))\n",
    "#     predictions.coalesce(1).write.format('com.databricks.spark.csv') \\\n",
    "#                .option('header','true') \\\n",
    "#                .save(validation_predictions_path)\n",
    "    predictions.write.saveAsTable(validation_predictions_tablename)\n",
    "    trained_owners_dict = trainingData.groupBy('owner').count().rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "    pkl.dump(trained_owners_dict, open(trained_owners_dict_path, 'wb'))\n",
    "    if evaluate_metrics:\n",
    "      metrics = evaluation_metrics(predictions)\n",
    "      actual_labels = predictions.select('owner').collect()\n",
    "      predicted_labels = predictions.select('ml_predicted_owner').collect()\n",
    "      classificationReport = classification_report(actual_labels, predicted_labels)\n",
    "      pkl.dump(metrics, open(metrics_path, 'wb'))\n",
    "      pkl.dump(classificationReport, open(classification_report_path, 'wb'))\n",
    "      results.update({'model': model_path, 'label_dict': label_dict_path, \\\n",
    "              'trained_owners_dict': trained_owners_dict_path, \\\n",
    "              'validation_predictions': validation_predictions_tablename, \\\n",
    "              'metrics': metrics_path, 'classificationReport': classification_report_path})\n",
    "      return results\n",
    "    else:\n",
    "      results.update({'model': model_path, 'label_dict': label_dict_path, \\\n",
    "              'trained_owners_dict': trained_owners_dict_path, \\\n",
    "              'validation_predictions': validation_predictions_tablename})\n",
    "      return results\n",
    "  else:\n",
    "    if hyperparameter_tuning:\n",
    "      trials, model_fitted = optimize_hyperopt([df])\n",
    "      trained_owners_dict = df.groupBy('owner').count().rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "      pkl.dump(trials, open(trials_path, 'wb'))\n",
    "      model_fitted.save(model_path)\n",
    "      pkl.dump(trained_owners_dict, open(trained_owners_dict_path, 'wb'))\n",
    "      results['training_accuracy'] = trials.best_trial['result']['loss']*-1\n",
    "      results.update({'model': model_path, 'label_dict': label_dict_path, \\\n",
    "              'trained_owners_dict': trained_owners_dict_path})\n",
    "      return results\n",
    "    elif train_existing_model:\n",
    "      model_fitted, accuracy = model_fit(df)\n",
    "      results['training_accuracy'] = accuracy\n",
    "      trained_owners_dict = df.groupBy('owner').count().rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "      model_fitted.save(model_path)\n",
    "      pkl.dump(trained_owners_dict, open(trained_owners_dict_path, 'wb'))\n",
    "      results.update({'model': model_path, 'label_dict': label_dict_path, \\\n",
    "              'trained_owners_dict': trained_owners_dict_path})\n",
    "      return results\n",
    "    else:\n",
    "      df.write.saveAsTable(transformed_df_path)\n",
    "      results.update({'transformed_df': transformed_df_path, 'label_dict': label_dict_path})\n",
    "      return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_pipeline(data, datestamp, model_path = '/dbfs/FileStore/models/tvs_lr_model_apr23_1.sav',\\\n",
    "                    label_dict_path = '/dbfs/FileStore/tables/label_mapping_apr23_1.pkl'):\n",
    "  '''\n",
    "  This is the main function to be called for executing the model testing pipeline. It takes \\\n",
    "  a PySpark dataframe which it transforms, passes through an existing model, and generates predictions\n",
    "   \n",
    "  Arguments: \n",
    "    data {PySpark Dataframe} -- A PySpark Dataframe containing feature vectors and labels\n",
    "    model_path {string:path} -- path in dbfs where the trained model is saved\n",
    "    label_dict_path {string: path} -- path in dbfs where the label-owner mapping dictionary is saved\n",
    "    \n",
    "  Returns:\n",
    "    results {tuple} -- A tuple containing (transformed model, predictions)\n",
    "  '''\n",
    "  \n",
    "#   testing_predictions_path = '/dbfs/dbfs/FileStore/predictions/testing_predictions_{}.csv'.format(datestamp)\n",
    "  testing_predictions_tablename = 'prediction_results_{}'.format(datestamp)\n",
    "  \n",
    "  input_features = list(set(data.columns) - set(['vulnerability_id']))\n",
    "  df = vector_assembler(input_features, data).select('features','vulnerability_id')\n",
    "  model = LogisticRegressionModel.load((model_path))\n",
    "  label_dict = pkl.load(open(label_dict_path, 'rb'))\n",
    "  predictions = model.transform(df)\n",
    "  \n",
    "  predictions = predictions.withColumn('ml_predicted_owner', udf(lambda x: label_dict[x])('prediction'))\n",
    "  predictions = predictions.withColumn('array_probs', udf(lambda v: v.toArray().tolist(), \\\n",
    "                                                      ArrayType(FloatType()))('probability'))\n",
    "  predictions = predictions.withColumn('confidence_factor', udf(lambda x: max(x), FloatType())('array_probs'))\n",
    "  predictions = predictions.withColumn('ml_owner',udf(lambda x: x)('ml_predicted_owner'))\n",
    "  predictions = predictions.withColumn('created_date',lit(datestamp).cast(LongType()))\n",
    "  \n",
    "  \n",
    "  predictions.select('vulnerability_id','ml_owner','ml_predicted_owner','confidence_factor','created_date').write.saveAsTable(testing_predictions_tablename)\n",
    "#   predictions.coalesce(1).write.format('com.databricks.spark.csv') \\\n",
    "#                .option('header','true') \\\n",
    "#                .save(testing_predictions_path)\n",
    "  \n",
    "  return {'predictions': testing_predictions_tablename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
