{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This a a notebook of helper functions htat will be called by the feature engieering notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, IndexToString, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, DenseVector, VectorUDT, _convert_to_vector\n",
    "from pyspark.sql import Row, types, Window\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "import scipy\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_featurized_df(data_path, new_col_names):\n",
    "  '''\n",
    "  This is a function that takes the path of a table stored on Databricks representing training\n",
    "  data, a list of new column names, and creates a PySpark dataframe. This is necessary because\n",
    "  the way column names are represented in our training data does not work with PySpark.\n",
    "   \n",
    "  Arguments: \n",
    "    data_path {string} -- String with the name of the table to be imported.\n",
    "    new_col_names {list of strings} -- A list of column names to be assigned to the Dataframe.\n",
    "    \n",
    "  Returns:\n",
    "    featurized {PySpark Dataframe} -- A PySpark Dataframe created from the data at data_path. \n",
    "      Columns are named in the order given in new_col_names.\n",
    "  '''\n",
    "  data = sqlContext.read.table(data_path)\n",
    "  old_col_names = data.schema.names\n",
    "  featurized = reduce(lambda data, idx: data.withColumnRenamed(old_col_names[idx], new_col_names[idx]), range(len(old_col_names)), data)\n",
    "  return featurized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevalence_plot_nvalue(col, df, prevalence_pct=1, display_img=True, fig_size=(8, 4)):\n",
    "  '''\n",
    "  This is a function that takes a Pyspark dataframe and a column name and returns the \n",
    "  number of features that must be retained to explain a user-defined amount of variance \n",
    "  in the column.\n",
    "  \n",
    "  Arguments: \n",
    "    col {string} -- The name of the column under consideration.\n",
    "    df {Pyspark Dataframe} -- The Pyspark Dataframe containing the column.\n",
    "    prevalence_pct {float} -- Threshold indicating the percentage of values be retained. \n",
    "      Unique values are counted and sorted in descending order, then cumulatively summed \n",
    "      until this percentage of entries from the data are included.\n",
    "    display_img {Boolean} -- Whether to display a plot of the cumulative sum. Default True.\n",
    "    fig_size {tuple of ints} -- The size of the matplotlib plot to be displayed.\n",
    "    \n",
    "  Returns:\n",
    "    n {int} -- The number of feature values to retain (of a descending sorted list of \n",
    "      feature prevalence.)\n",
    "  '''\n",
    "  col_series = df.groupBy(col).count().sort(F.desc(\"count\"))\n",
    "  total_sum = col_series.agg({\"count\":\"sum\"}).collect()[0][0]\n",
    "  col_series = col_series.withColumn('variances', udf(lambda x: int(x)/total_sum)('count'))\n",
    "  windowval = (Window.orderBy(F.col('count').desc()).rangeBetween(Window.unboundedPreceding, 0))\n",
    "  col_series = col_series.withColumn('cum_sum', F.sum('variances').over(windowval))\n",
    "  n = col_series.filter(F.col('cum_sum') <= prevalence_pct).count() \n",
    "  f, ax = plt.subplots(figsize=fig_size)\n",
    "  ax.plot(col_series.select('cum_sum').collect())\n",
    "  ax.vlines(n, 0, 1, linestyles='dashed')\n",
    "  ax.annotate(\n",
    "    '{}% Prevalence: N = {}'.format(prevalence_pct, n), \n",
    "    xy=(n, prevalence_pct), \n",
    "    xycoords='data', \n",
    "    xytext=(n * 3, prevalence_pct - .1), \n",
    "    arrowprops=dict(\n",
    "      facecolor='black', \n",
    "      headwidth=10, \n",
    "      headlength=7, \n",
    "      width=1, \n",
    "      shrink=.05\n",
    "    )\n",
    "  )\n",
    "  ax.set_xlabel('Number of Features')\n",
    "  ax.set_ylabel('Prevalence Covered')\n",
    "  f.suptitle('{} prevalence covered by top N features'.format(col))\n",
    "\n",
    "  if display_img:\n",
    "    display(f)\n",
    "  return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_values(n, col, df):\n",
    "  '''\n",
    "  A function to return a list of the n most prevalent values in a given column.\n",
    "  Used after prevalence_plot_nvalue has returned the number of feature values to \n",
    "  retain.\n",
    "  \n",
    "  Arguments: \n",
    "    n {integer} -- The number of values to return.\n",
    "    col {string} -- The name of the column under consideration.\n",
    "    df {Pyspark Dataframe} -- The Pyspark Dataframe containing the column.\n",
    "    \n",
    "  Returns:\n",
    "    final_values {list} -- A list of strings representing the values to retain\n",
    "    from the column.\n",
    "  '''\n",
    "  col_series = df.groupBy(col).count().sort(F.desc(\"count\"))\n",
    "  vals = col_series.filter(~F.col(col).isin('nan','')).head(n)\n",
    "  final_values = [x[0] for x in vals]\n",
    "  return final_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_col_values(val, final_values, fill=''):\n",
    "  '''\n",
    "  A UDF used to retain the values returned by prevalence_plot_nvalue and\n",
    "  get_n_values and to replace others with empty strings.\n",
    "  \n",
    "  Arguments: \n",
    "    val {string} -- The string being considered for retention.\n",
    "    final_values {list of strings} -- The list of value strings returned\n",
    "      by get_n_values.\n",
    "    \n",
    "  Returns:\n",
    "    result {string} -- A string representing the value.\n",
    "  '''\n",
    "  result = val if val in final_values else fill\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(text, vectors):\n",
    "  '''\n",
    "  A UDF used to take strings and return their vector representation.\n",
    "  \n",
    "  Arguments: \n",
    "    text {string} -- The string being considered for retention.\n",
    "    vectors {Gensim FastTextKeyedVectors} -- FastTextKeyedVectors\n",
    "      trained by Gensim\n",
    "    \n",
    "  Returns:\n",
    "    result {list of floats} -- List of floats representing a vector\n",
    "      embedding (or 0.0s if text is an empty string).\n",
    "  '''\n",
    "  if text == '':\n",
    "    return [float(0) for x in range(vectors.vector_size)]\n",
    "  else:\n",
    "    vector = vectors[text]\n",
    "    return [float(x) for x in vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_count_vectorize_column(data, input_col, output_col, create=True):\n",
    "  '''\n",
    "  A function to binary count vectorize a given column. This reduces lines of code compared to explicitly\n",
    "  doing this for each relevant feature. It also saves the fit model so it can be called from the \n",
    "  notebook that does feature engineering for prediction.\n",
    "     \n",
    "  Arguments: \n",
    "    data {PySpark Dataframe} -- The dataframe having a binary count vectorize column added.\n",
    "    input_col {string} -- The name of the column being binary count vectorized.\n",
    "    output_col {string} -- The name of the new column.\n",
    "    \n",
    "  Returns:\n",
    "    featurized {PySpark Dataframe} -- The provided dataframe with a new column added.\n",
    "  '''\n",
    "  temp_cv = CountVectorizer(inputCol=input_col, outputCol=output_col, binary=True)\n",
    "  temp_model = temp_cv.fit(data)\n",
    "  temp_model.write().overwrite().save('{}_model'.format(input_col))\n",
    "  featurized = temp_model.transform(data)\n",
    "  return featurized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_sparse(array):\n",
    "  '''\n",
    "  A UDF to convert an array of strings into a sparse vector, which is the format required\n",
    "  in our model pipeline. \n",
    "     \n",
    "  Arguments: \n",
    "    array {array} -- An array of being converted into a sparse vector.\n",
    "    \n",
    "  Returns:\n",
    "    A sparse vector.\n",
    "  '''\n",
    "  dv = DenseVector(array)\n",
    "  return _convert_to_vector(scipy.sparse.csc_matrix(dv.toArray()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_training_pipeline(datestamp, data_tablename, cols=None, display=False, vector_len=100):\n",
    "  '''\n",
    "  A function to run the entire feature engineering for training pipeline. \n",
    "     \n",
    "  Arguments: \n",
    "    datestamp {string} -- Today's date in MMDDYY format. Used to name the table.\n",
    "    data_tablename {string} -- The name of the table of raw data used as input.\n",
    "    cols {list of strings} -- a list of column names to be used if the schema changes.\n",
    "    display {Boolean} -- Whether to display plots when prevalence_plot_nvalue is \n",
    "      called. Default false.\n",
    "    vector_len {integer} -- Length of Fasttext vectors to create. Default 100.\n",
    "    \n",
    "  Returns:\n",
    "    A string representing the name of the feature engineered dataset that has\n",
    "      been saved.\n",
    "  '''\n",
    "  if cols is None:\n",
    "    col_list = ['vulnerability_id',\n",
    "    'assetName',\n",
    "    'country',\n",
    "    'status',\n",
    "    'priority',\n",
    "    'dataSource',\n",
    "    'dnsName',\n",
    "    'operatingSystem',\n",
    "    'ipAdress',\n",
    "    'findingTitle',\n",
    "    'network',\n",
    "    'port',\n",
    "    'protocol',\n",
    "    'netBios',\n",
    "    'issue_name',\n",
    "    'owner',\n",
    "    'manager',\n",
    "    'director',\n",
    "    'vicePresident',\n",
    "    'ml_owner', \n",
    "    'ml_predicted_owner', \n",
    "    'ownership_source', \n",
    "    'report_name'\n",
    "               ]\n",
    "  else:\n",
    "    col_list = cols\n",
    "  \n",
    "  # Create a dataframe using the data with properly formatted names assigned.\n",
    "  featurized = create_featurized_df(data_tablename, col_list)\n",
    "  \n",
    "  # Split the IP Address column into 4 separate columns.\n",
    "  split_col = F.split(featurized['ipAdress'], '[.]')\n",
    "  featurized = featurized.withColumn('ipAdress_1', split_col.getItem(0))\n",
    "  featurized = featurized.withColumn('ipAdress_2', split_col.getItem(1))\n",
    "  featurized = featurized.withColumn('ipAdress_3', split_col.getItem(2))\n",
    "  featurized = featurized.withColumn('ipAdress_4', split_col.getItem(3))\n",
    "  \n",
    "  # Select just the columns that we are interested in.\n",
    "  featurized = featurized.select(\n",
    "  'ipAdress_1',\n",
    "  'ipAdress_2',\n",
    "  'ipAdress_3',\n",
    "  'ipAdress_4',\n",
    "  'country',\n",
    "  'dataSource',\n",
    "  'dnsName',\n",
    "  'operatingSystem',\n",
    "  'network',\n",
    "  'port',\n",
    "  'protocol',\n",
    "  'issue_name',\n",
    "  'owner',\n",
    "  'vulnerability_id'\n",
    "  )\n",
    "  \n",
    "  # Run prevalence plot feature selection steps for issue_name, port, operatingSystem\n",
    "  # issue_name\n",
    "  feats = prevalence_plot_nvalue(\"issue_name\", featurized, display_img=display)\n",
    "  final_values = get_n_values(feats, \"issue_name\", featurized)\n",
    "  featurized = featurized.withColumn('issue_name_new', udf(lambda x: new_col_values(x, final_values))(F.col('issue_name')))\n",
    "  \n",
    "  # port\n",
    "  feats = prevalence_plot_nvalue('port', featurized, display_img=display)\n",
    "  final_values = get_n_values(feats, 'port', featurized)\n",
    "  featurized = featurized.withColumn('port_new', udf(lambda x: new_col_values(x, final_values))(F.col(\"port\")))\n",
    "  \n",
    "  # operatingSystem\n",
    "  # We're using the first two words describing the operating system.\n",
    "  featurized = featurized.withColumn('operatingSystem_int', udf(lambda x: ' '.join(str(x).split()[:2]))(F.col(\"operatingSystem\")))\n",
    "  \n",
    "  feats = prevalence_plot_nvalue('operatingSystem_int', featurized, display_img=display)\n",
    "  final_values = get_n_values(feats, 'operatingSystem_int', featurized)\n",
    "  featurized = featurized.withColumn('operatingSystem_new', udf(lambda x: new_col_values(x, final_values))(F.col(\"operatingSystem_int\")))\n",
    "  \n",
    "  # owner (add a new column that labels the 1% of least common owners all as a class 'unknown')\n",
    "  feats = prevalence_plot_nvalue('owner', featurized, prevalence_pct=.99, display_img=False)\n",
    "  final_values = get_n_values(feats, 'owner', featurized)\n",
    "  featurized = featurized.withColumn('owner_new', udf(lambda x: new_col_values(x, final_values, fill='unknown'))(F.col(\"owner\")))\n",
    "  \n",
    "  # Split dnsName into component parts, then encode them using Fasttext\n",
    "  featurized = featurized.withColumn('dns_domain', udf(lambda x: tldextract.extract(str(x)).domain)(F.col('dnsName')))\n",
    "  featurized = featurized.withColumn('dns_suffix', udf(lambda x: tldextract.extract(str(x)).suffix)(F.col('dnsName')))\n",
    "  featurized = featurized.withColumn('dns_subdomain', udf(lambda x: tldextract.extract(str(x)).subdomain)(F.col('dnsName')))\n",
    "  \n",
    "  # Identify rows where `dns_subdomain` is missing.\n",
    "  featurized = featurized.withColumn('dns_subdomain_isnull', udf(lambda x: 1 if x == '' else 0)(F.col('dns_subdomain')))\n",
    "  \n",
    "  # Create a list of all subdomains to use as our fasttext vocabulary.\n",
    "  dns_subdomain_list = [[row.dns_subdomain] for row in featurized.select('dns_subdomain').where(featurized.dns_subdomain != '').collect()]\n",
    "  \n",
    "  # Create fasttext word embeddings.\n",
    "  vector_size = vector_len\n",
    "  model = gensim.models.FastText(size=vector_size)\n",
    "  model.build_vocab(sentences=dns_subdomain_list)\n",
    "  model.train(sentences=dns_subdomain_list, total_examples=len(dns_subdomain_list), epochs=10)\n",
    "  dns_vectors = model.wv\n",
    "  \n",
    "  # Save out the KeyedVectors\n",
    "  fname = get_tmpfile('/dbfs/fasttext_dns_vectors_{}.kv'.format(vector_size))\n",
    "  dns_vectors.save(fname)\n",
    "  \n",
    "  # Retrieve trained vectors of subdomains and create a new column.\n",
    "  featurized = featurized.withColumn('dns_subdomain_vector', udf(lambda x: get_embedding_vectors(x, dns_vectors), ArrayType(FloatType()))(F.col('dns_subdomain')))\n",
    "  \n",
    "  # One Hot Encode the other features.\n",
    "  featurized = featurized.select(\n",
    "    'owner', \n",
    "    'vulnerability_id', \n",
    "    'ipAdress_1', \n",
    "    'ipAdress_2', \n",
    "    'ipAdress_3', \n",
    "    'ipAdress_4', \n",
    "    'issue_name_new', \n",
    "    'port_new', \n",
    "    'operatingSystem_new', \n",
    "    'country', \n",
    "    'protocol', \n",
    "    'dataSource', \n",
    "    'network', \n",
    "    'dns_subdomain_vector', \n",
    "    'dns_domain', \n",
    "    'dns_suffix', \n",
    "    'dns_subdomain_isnull'\n",
    "  )\n",
    "  \n",
    "  # These are the columns that actually need to be vecorized - dns_subdomain_vector is already a vector, \n",
    "  # owner is the target variable, and vulnerability_id is supposed to just be passed through.\n",
    "  vectorize_cols = list(set(featurized.columns) - set(['dns_subdomain_vector', 'owner', 'vulnerability_id']))\n",
    "  \n",
    "  # In order to get count vectorized, every non-array feature needs to be turned into one.\n",
    "  for col in vectorize_cols:\n",
    "    featurized = featurized.withColumn(col, F.array(featurized[col]))\n",
    "    featurized = binary_count_vectorize_column(featurized, col, col + '_vectors')\n",
    "    \n",
    "  # This step converts the dns_subdomain_vector array to a sparse array udt so it'll work later with our model pipeline.\n",
    "  featurized = featurized.withColumn('dns_subdomain_vector', udf(dense_to_sparse, VectorUDT())(F.col('dns_subdomain_vector')))\n",
    "  \n",
    "  # Select the relevant columns \n",
    "  featurized = featurized.select(\n",
    "    'dns_subdomain_vector', \n",
    "    'ipAdress_1_vectors', \n",
    "    'ipAdress_2_vectors',   \n",
    "    'ipAdress_3_vectors',   \n",
    "    'ipAdress_4_vectors',   \n",
    "    'issue_name_new_vectors',   \n",
    "    'port_new_vectors',   \n",
    "    'operatingSystem_new_vectors',   \n",
    "    'country_vectors',   \n",
    "    'protocol_vectors',  \n",
    "    'dataSource_vectors',   \n",
    "    'network_vectors',   \n",
    "    'dns_domain_vectors',   \n",
    "    'dns_suffix_vectors',  \n",
    "    'dns_subdomain_isnull_vectors',  \n",
    "    'owner',\n",
    "    'vulnerability_id'\n",
    "  )\n",
    "  \n",
    "  # Write out the table.\n",
    "  output_table_name = 'ads_training_embedded_dns_len_{}_{}'.format(vector_size, datestamp)\n",
    "  featurized.write.saveAsTable(output_table_name)\n",
    "  \n",
    "  return output_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
